{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kevinscaria/InstructABSA/blob/main/JointTask_Training_%26_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2xc1EQjq3LY"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4oIyN4lu6qm",
    "outputId": "cf2db64e-cc0f-4682-c372-3cae89f597ee"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTZzGWQTQAxt"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install transformers\n",
    "  !pip install datasets\n",
    "  !pip install evaluate\n",
    "  !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oTPPfF2q5S5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "if IN_COLAB:\n",
    "    root_path = 'Enter drive path'  ### <-- Enter your drive path\n",
    "else:\n",
    "    root_path = 'Enter local path'  ### <-- Or enter your local path\n",
    "    \n",
    "use_mps = True if torch.has_mps else False\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DvLkxG9lp2t"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "from InstructABSA.data_prep import DatasetLoader\n",
    "from InstructABSA.utils import T5Generator, T5Classifier\n",
    "from instructions import InstructionsHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht-x1qxn_AxG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knn9pcJMXovr"
   },
   "outputs": [],
   "source": [
    "task_name = 'joint_task'\n",
    "experiment_name = 'your_experiment_name'  ### <-- Enter your experiment name\n",
    "model_checkpoint = 'kevinscaria/joint_tk-instruct-base-def-pos-neg-neut-combined'  # The updated joint model\n",
    "print('Experiment Name: ', experiment_name)\n",
    "model_out_path = './Models'\n",
    "model_out_path = os.path.join(model_out_path, task_name, f\"{model_checkpoint.replace('/', '')}-{experiment_name}\")\n",
    "print('Model output path: ', model_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hfAuLAvVe8D"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/Train/Laptops_Train.csv'  ### <-- Update with your data\n",
    "id_test_file_path = './Dataset/SemEval14/Test/Laptops_Test.csv'   ### <-- Update with your data\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set2()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_aspe_task_format(loader.train_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_aspe_task_format(loader.test_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4v8QMMT4B7t"
   },
   "outputs": [],
   "source": [
    "# Create T5 utils object\n",
    "t5_exp = T5Generator(model_checkpoint)\n",
    "\n",
    "# Tokenize Dataset\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenized_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Training arguments\n",
    "training_args = {\n",
    "    'output_dir':model_out_path,\n",
    "    'evaluation_strategy':\"epoch\",\n",
    "    'learning_rate':5e-5,\n",
    "    'lr_scheduler_type':'cosine',\n",
    "    'per_device_train_batch_size':8,\n",
    "    'per_device_eval_batch_size':16,\n",
    "    'num_train_epochs':4,\n",
    "    'weight_decay':0.01,\n",
    "    'warmup_ratio':0.1,\n",
    "    'save_strategy':'no',\n",
    "    'load_best_model_at_end':False,\n",
    "    'push_to_hub':False,\n",
    "    'eval_accumulation_steps':1,\n",
    "    'predict_with_generate':True,\n",
    "    'use_mps_device':use_mps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZr7BAy6eJOA"
   },
   "outputs": [],
   "source": [
    "id_tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It'll give us an error if we don't have a validation set (when we try to train). Let's create one now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test dataset in half\n",
    "train_test_split = id_tokenized_ds['test'].train_test_split(test_size=0.5)\n",
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the \"new\" 'train' and 'test' splits to the original DatasetDict with their new names\n",
    "id_tokenized_ds['test'] = train_test_split['train']\n",
    "id_tokenized_ds['validation'] = train_test_split['test']  # Use 'test' as the validation set\n",
    "id_tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_trainer = t5_exp.train(id_tokenized_ds, **training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rqL4maz_Dlz"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy6aOHv4_FUo"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/Train/Laptops_Train.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/Test/Laptops_Test.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set2()  # same as above, change to instruction_set2\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "# Same as above. 'joint' changed to 'aspe' in the code below\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_aspe_task_format(loader.train_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_aspe_task_format(loader.test_df_id, 'term', 'polarity', 'raw_text', 'aspectTerms', instruct_handler.aspe['bos_instruct2'], instruct_handler.aspe['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbcnbLXtt9Am"
   },
   "outputs": [],
   "source": [
    "# Model inference - Loading from Checkpoint\n",
    "t5_exp = T5Generator(model_out_path)\n",
    "\n",
    "# Tokenize Datasets\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Get prediction labels - Training set   \n",
    "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16)\n",
    "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
    "\n",
    "# Get prediction labels - Testing set\n",
    "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test', batch_size = 16)\n",
    "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBCUT9jDt8-Q"
   },
   "outputs": [],
   "source": [
    "p, r, f1, _ = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
    "print('Train Precision: ', p)\n",
    "print('Train Recall: ', r)\n",
    "print('Train F1: ', f1)\n",
    "\n",
    "p, r, f1, _ = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
    "print('Test Precision: ', p)\n",
    "print('Test Recall: ', r)\n",
    "print('Test F1: ', f1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "070d57aa6b4a039a680ca3535d2f37da5ed020b02d8ccf58fedcd4e32b3636b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
